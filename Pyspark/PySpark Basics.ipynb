{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3ef80a5-aaac-464e-86b5-98bd3ba52a93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#RDD\n",
    "#from pyspark.sql import SparkSession\n",
    "#spark=SparkSession.builder.appName(\"MyApp\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44755802-ce8c-4287-9b8d-871d3594a24f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data=[1,2,3,4,5]\n",
    "rdd=spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "132a59f8-1142-4b13-98f8-8def753c956f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "979b5510-d6e8-4d11-8565-abfe4ea7cafa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "575aa78e-d576-4362-ae14-499784bb8321",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Method1\n",
    "emp_data= [\n",
    "    (101,\"Sachin\",20000,10),\n",
    "    (102,\"Rahul\",30000,10),\n",
    "    (103,\"Sourav\",40000,20),\n",
    "    (104,\"Santhosh\",50000,20)\n",
    "]\n",
    "columns=[\"emp_id\",\"emp_name\",\"emp_salary\",\"dept_id\"]\n",
    "emp_df=spark.createDataFrame(data=emp_data,schema=columns)\n",
    "display(emp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6c5821d-38c5-4f83-834d-1a2b67915816",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# to see output in spark df\n",
    "emp_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93310b34-4e3f-4942-bd0b-2f1b6e06919b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Method 2\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructField, StructType,IntegerType,StringType\n",
    "emp_data=[\n",
    "    (101,\"Sachin\",20000,10),\n",
    "    (102,\"Rahul\",30000,10),\n",
    "    (103,\"Sourav\",40000,20),\n",
    "    (104,\"Santhosh\",50000,20)\n",
    "]\n",
    "schema1=StructType([\n",
    "    StructField(\"emp_id\",IntegerType(),False),\n",
    "    StructField(\"emp_name\",StringType(),True),\n",
    "    StructField(\"emp_salary\",IntegerType(),True),\n",
    "    StructField(\"dept_id\",IntegerType(),True)\n",
    "])\n",
    "emp_df2=spark.createDataFrame(data=emp_data,schema=schema1)\n",
    "display(emp_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "838bfecf-e6b6-49b8-9c7e-9e149960f6bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#to convert dataframes into RDD to see partitions\n",
    "emp_df2.rdd.getNumPartitions()\n",
    "#[NOT_IMPLEMENTED] Using custom code using PySpark RDDs is not allowed on serverless compute. We suggest using mapInPandas or mapInArrow for the most common use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fbdfb30-1443-4a1f-b693-c86f7d216b97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# adding new column to the existing df\n",
    "emp_df2= emp_df2.withColumn(\"bonus\",F.col(\"emp_salary\")*0.1)\n",
    "display(emp_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b57e947-c0a7-4f29-92d1-4fd0fe76e2b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#to add multiple columns to the existing df instead of withcoloumn we use withcolumns\n",
    "emp_df2= emp_df2.withColumns({\n",
    "    \"dummy1\":F.lit(100),\n",
    "    \"dummy2\":F.lit(100),\n",
    "    \"date\":F.current_date()\n",
    "})\n",
    "display(emp_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9061932-1c1a-452e-92b7-9b3e374ed360",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bb2e36a-a0bd-4589-ac0a-19cf4db3b7c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_df2_dropped=emp_df2.drop(\"bonus1\",\"bonus2\",)\n",
    "display(emp_df2_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "875255f0-442f-481d-92a2-8b364b967f4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d58bb614-fb4a-4482-a462-ae6dbd627f81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register the DataFrame as a temporary view\n",
    "emp_df2.createOrReplaceTempView(\"emp_df2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01de80a8-14d7-4768-8a92-493944e06587",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save as a managed table\n",
    "emp_df2.write.mode(\"overwrite\").saveAsTable(\"default.emp_df2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0560ee5-6dcc-4f30-ab06-f2597c6f2818",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# You must enable Delta column mapping before dropping columns. Run the following command to enable column mapping, then drop the columns as needed.\n",
    "\n",
    "\n",
    "%sql\n",
    "ALTER TABLE default.emp_df2 SET TBLPROPERTIES (\n",
    "  'delta.columnMapping.mode' = 'name'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4b51852-8954-4549-a88d-c7c17a06aee7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# After enabling column mapping, you can drop columns using:\n",
    "%sql\n",
    "ALTER TABLE default.emp_df2 DROP COLUMN bonus1, bonus2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b080c71b-5b7c-4f99-91d0-8eec631cac98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# to see the table structure\n",
    "spark.table(\"default.emp_df2\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9d901f5-5dc9-4490-9b67-8bfbdede4a84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#we reloaded dataframe after dropping the columns\n",
    "emp_df2=spark.table(\"default.emp_df2\")\n",
    "display(emp_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58b48966-495a-45d3-bdbf-2e4c14eb3c1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# we cross checked the columns once after deleting\n",
    "emp_df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbdbbd72-ddd5-4c04-8e0f-8e55a1bce13d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# rename the column from emp_salary to salary\n",
    "emp_df2=emp_df2.withColumnRenamed(\"emp_salary\",\"Salary\")\n",
    "emp_df2.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3de49c84-e151-494e-a8bb-bb238661b543",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#filtering the columns\n",
    "\n",
    "emp_filtered=emp_df2.filter(F.col(\"dept_id\")==20)\n",
    "display(emp_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4d0bc1f-1485-42d7-b0e7-ac93f019ad22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#filtering by using where\n",
    "\n",
    "emp_filtered=emp_df2.where(F.col(\"dept_id\")==10)\n",
    "display(emp_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be132ade-6879-4f48-8d85-c130cb780dc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# different ways to select columns in spark df\n",
    "\n",
    "#Method1\n",
    "display(emp_df2.select(\"emp_id\",\"emp_name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7a276fe-bb0e-4789-a3d0-b85ac79607d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Method2\n",
    "display(emp_df2.select(emp_df2[\"emp_id\"],emp_df2[\"emp_name\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa4359af-3db2-4089-813f-8b4944a8e8fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Method 3\n",
    "display(emp_df2.select(emp_df2.emp_id,emp_df2.emp_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1452230b-58a4-4be4-a8e3-a729a51bf466",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Method 4\n",
    "display(emp_df2.select(F.col(\"emp_id\"),F.col(\"emp_name\")))   "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4591296585232902,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark Basics",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
